import requests
import pandas as pd
import time
import json
import os
import math
import numpy as np
import ast
import seaborn as sns
import matplotlib as plt 
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
import networkx as nx
from networkx.algorithms import bipartite 

def query_disgenet_all_pages(gene_id, api_key, output_folder):
    
    base_url = "https://api.disgenet.com/api/v1/gda/summary"

    headers = {
        'Authorization': api_key,
        'accept': 'application/json'
    }

    all_results = []
    page_number = 0
    page_size = 100  # DisGeNET API'nin maksimum desteklediÄŸi sayfa boyutu bu, 100 uygundur.

    # Ä°lk sorgu: KaÃ§ kayÄ±t olduÄŸunu Ã¶ÄŸrenmek iÃ§in
    params = {
        "gene_ncbi_id": gene_id,
        "page_number": page_number,
        "page_size": page_size
    }

    while True:
        response = requests.get(base_url, params=params, headers=headers, verify=False)

        if response.status_code == 429:
            wait_time = int(response.headers.get('x-rate-limit-retry-after-seconds', 5))
            print(f"Rate limit aÅŸÄ±ldÄ±. {wait_time} saniye bekleniyor...")
            time.sleep(wait_time)
            continue

        if not response.ok:
            print(f"Hata oluÅŸtu! Status code: {response.status_code}")
            return None

        break

    data = response.json()
    total_elements = data.get("paging", {}).get("totalElements", 0)
    total_pages = math.ceil(total_elements / page_size)

    print(f"\nGen {gene_id} iÃ§in toplam {total_elements} iliÅŸki bulundu. Toplam sayfa: {total_pages}")

    # Ä°lk sayfadaki veriyi ekle
    all_results.extend(data.get("payload", []))

    # Kalan sayfalarÄ± Ã§ek
    for page in range(1, total_pages):
        params["page_number"] = page
        while True:
            response = requests.get(base_url, params=params, headers=headers, verify=False)

            if response.status_code == 429:
                wait_time = int(response.headers.get('x-rate-limit-retry-after-seconds', 5))
                print(f"Rate limit aÅŸÄ±ldÄ±. {wait_time} saniye bekleniyor...")
                time.sleep(wait_time)
                continue

            if not response.ok:
                print(f"Hata oluÅŸtu! Status code: {response.status_code}")
                break

            break

        page_data = response.json()
        all_results.extend(page_data.get("payload", []))
        time.sleep(0.3) 
    print(f"Toplam Ã§ekilen kayÄ±t: {len(all_results)}")

    if not all_results:
        print(f"Gen {gene_id} iÃ§in iliÅŸki bulunamadÄ±.")
        return None

    df = pd.DataFrame(all_results)
    os.makedirs(output_folder, exist_ok=True)

    csv_filename = os.path.join(output_folder, f"gene_{gene_id}.csv")
    json_filename = os.path.join(output_folder, f"gene_{gene_id}.json")

    df.to_csv(csv_filename, index=False)
    print(f"{len(df)} kayÄ±t CSV dosyasÄ±na yazÄ±ldÄ±: {csv_filename}")

    with open(json_filename, "w", encoding="utf-8") as f:
        json.dump({"payload": all_results}, f, ensure_ascii=False, indent=4)
    print(f"JSON dosyasÄ± kaydedildi: {json_filename}")

    return df

# Toplu iÅŸleyen kÄ±sÄ±m
def batch_all_genes(gene_file_path, api_key, output_folder):
    df = pd.read_csv(gene_file_path)
    
    total_gen_count = len(df)
    print(f"Toplam {total_gen_count} gen bulundu.")
    
    # 'geneID' kolonunu okuyup listeye Ã§eviriyoruz
    gene_ids = df['geneID'].tolist()

    print(f"Toplam {len(gene_ids)} gen bulunuyor. Veri toplanÄ±yor...")

    for idx, gene_id in enumerate(gene_ids, 0):
        #print(f"\n[{idx}/{len(gene_ids)}] Gen ID: {gene_id} iÅŸleniyor...")
        print(f"\n[{idx}/{total_gen_count}] Gen ID: {gene_id} iÅŸleniyor...")
        try:
            query_disgenet_all_pages(gene_id, api_key, output_folder)
            time.sleep(0.5)  # API'yi Ã§ok yormamak iÃ§in kÃ¼Ã§Ã¼k gecikme
        except Exception as e:
            print(f"{gene_id} iÃ§in hata oluÅŸtu: {e}")
            break
def data_filtering(df):
    # Filtreleme
    filtered_df = df[(df['ei'] >= 0.6) & (df['score'] >= 0.4) & (df['numPMIDs'] >= 2)]

    # Kalan veri sayÄ±sÄ±
    print("FiltrelenmiÅŸ veri sayÄ±sÄ±:", len(filtered_df))

    # Kaydetme
    filtered_df.to_csv("filtered_dataset_0.4.csv", index=False)

def feature_select(df):
    columns_to_keep = [
        "symbolOfGene",
        "geneNcbiID",
        "diseaseUMLSCUI",
        "diseaseName",
        "diseaseClasses_DO",
        "diseaseClasses_MSH",
        "diseaseClasses_UMLS_ST",
        "diseaseClasses_HPO",
        "score",
        "ei",
        "numPMIDs"
    ]
    
    available_cols = [col for col in columns_to_keep if col in df.columns]
    return df[available_cols]

def merge_selected_features(folder_path, output_file):
    all_data = []
    
    for filename in os.listdir(folder_path):
        if filename.endswith(".csv"):
            file_path = os.path.join(folder_path, filename)
            try:
                df = pd.read_csv(file_path)
                df_selected = feature_select(df)
                all_data.append(df_selected)
            except Exception as e:
                print(f"{filename} okunurken hata oluÅŸtu: {e}")
    if all_data:
        merged_df = pd.concat(all_data, ignore_index=True)
        merged_df.to_csv(output_file, index=False)
        print(f"TÃ¼m veriler '{output_file}' dosyasÄ±na kaydedildi. Toplam kayÄ±t sayÄ±sÄ±: {len(merged_df)}")
    else:
        print("HiÃ§bir dosya okunamadÄ± veya uygun veri bulunamadÄ±.")

def clean_null(file_path, cleaned_file, null_file):
    """
    TÃ¼m kolonlarda eksik (NaN veya '[]') verileri tespit eder.
    Eksik ve eksiksiz kayÄ±tlarÄ± ayrÄ± dosyalara kaydeder.
    Her kolon iÃ§in eksik olup olmadÄ±ÄŸÄ±nÄ± if-else yapÄ±sÄ±yla raporlar.
    """

    df = pd.read_csv(file_path)

    eksik_mask = df.isna() | (df == "[]")
    is_null = np.any(eksik_mask.values, axis=1)

    null_df = df[is_null]
    clean_df = df[~is_null]

    print("\nKolon bazÄ±nda eksik kontrolÃ¼:\n")
    for col in df.columns:
        null_count = df[col].isna().sum() + (df[col] == "[]").sum()
        
        if null_count > 0:
            print(f"{col} kolonunda {null_count} eksik veya [] deÄŸer var.")
        else:
            print(f"{col} kolonunda eksik veya [] deÄŸer YOK.")

    if len(null_df) > 0:
        null_df.to_csv(null_file, index=False)
        print(f"\n{len(null_df)} eksik kayÄ±t '{null_file}' dosyasÄ±na kaydedildi.")
    else:
        print("\nEksik kayÄ±t bulunmadÄ±. Eksik dosyasÄ± oluÅŸturulmadÄ±.")

    if len(clean_df) > 0:
        clean_df.to_csv(cleaned_file, index=False)
        print(f"{len(clean_df)} temiz kayÄ±t '{cleaned_file}' dosyasÄ±na kaydedildi.")
    else:
        print("Temiz kayÄ±t bulunmadÄ±.")

# Liste stringlerini dÃ¼zleÅŸtiren fonksiyon
def flatten_list_string(s):
    try:
        liste = ast.literal_eval(s)
        if isinstance(liste, list):
            return ", ".join(liste)
    except:
        pass
    return s

def convert_numeric(df):
    numeric_cols = ["score", "ei", "numPMIDs"]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")
    return df 

def clean_text(df):
    if "diseaseName" in df.columns:
        df["diseaseName"] = df["diseaseName"].str.lower().str.replace(r"[^\w\s]", "", regex=True)
    return df

def preprocess_pipeline(df):
    """
    liste_kolonlar = ["diseaseClasses_DO", "diseaseClasses_MSH", "diseaseClasses_UMLS_ST", "diseaseClasses_HPO"]

    for col in liste_kolonlar:
        if col in df.columns:
            df[col] = df[col].replace("[]", "unknown")
            df[col] = df[col].apply(flatten_list_string)
     """
    #df = convert_numeric(df)
    df = clean_text(df)        
    return df  

def data_analysis(df):
    
    # SayÄ±sal alanlarÄ±n genel daÄŸÄ±lÄ±mÄ±
    print(df.describe())

    # Eksik veya unknown oranlarÄ±
    print((df.isna().sum() / len(df)) * 100)
    print((df == "unknown").sum())
    
    # Score daÄŸÄ±lÄ±mÄ±
    sns.histplot(df["score"], bins=20, kde=True)
    plt.title("Score DaÄŸÄ±lÄ±mÄ±")
    plt.show()

    # ei daÄŸÄ±lÄ±mÄ±
    sns.histplot(df["ei"], bins=20, kde=True)
    plt.title("ei DaÄŸÄ±lÄ±mÄ±")
    plt.show()

    
    # SÄ±k geÃ§en diseaseClasses_DO ilk 20
    print(df["diseaseClasses_DO"].value_counts().head(20))
    
    # SÄ±k geÃ§en diseaseName ilk 20
    top_diseases = df["diseaseName"].value_counts().head(20)
    print(top_diseases)
    sns.barplot(x=top_diseases.values, y=top_diseases.index)
    plt.title("En SÄ±k GeÃ§en DiseaseName'ler")
    plt.xlabel("Frekans")
    plt.ylabel("HastalÄ±k")
    plt.show()

    #yoÄŸunluk haritasÄ± 
    plt.figure(figsize=(8,6))
    sns.kdeplot(data=df, x="score", y="ei", fill=True, cmap="Blues", thresh=0.05)
    plt.title("Score - ei YoÄŸunluk HaritasÄ±")
    plt.xlabel("Score")
    plt.ylabel("ei")
    plt.show()
    
    #hexbin plot altÄ±gen yoÄŸunluk grafiÄŸi
    plt.figure(figsize=(8,6))
    plt.hexbin(df["score"], df["ei"], gridsize=25, cmap="Purples", mincnt=1)
    plt.colorbar(label="Veri YoÄŸunluÄŸu")
    plt.title("Score - ei AltÄ±gen YoÄŸunluk HaritasÄ±")
    plt.xlabel("Score")
    plt.ylabel("ei")
    plt.show()
    
    corr = df["score"].corr(df["ei"])
    print(f"Score ile ei arasÄ±ndaki Pearson korelasyonu: {corr:.3f}")
    
    corr = df[["score", "ei", "numPMIDs"]].corr()
    sns.heatmap(corr, annot=True, cmap="coolwarm")
    plt.title("Korelasyon Matrisi")
    plt.show()

    #daÄŸÄ±lÄ±m grafiÄŸi scatterplot+renk kodlu yoÄŸunluk
    df["log_numPMIDs"] = np.log1p(df["numPMIDs"])
    
    plt.figure(figsize=(8,6))
    # sns.scatterplot(data=df, x="score", y="ei", hue="numPMIDs", palette="viridis", size="log_numPMIDs", sizes=(20, 200))
    sns.scatterplot(data=df, x="score", y="ei", hue="log_numPMIDs", palette="viridis", size="log_numPMIDs", sizes=(20, 200))
    sns.scatterplot(data=df, x="score", y="ei", size="log_numPMIDs", sizes=(20, 200))
    plt.title("Score - ei DaÄŸÄ±lÄ±mÄ± (log(numPMIDs) gÃ¶re renk ve bÃ¼yÃ¼klÃ¼k)")
    plt.xlabel("Score")
    plt.ylabel("ei")
    plt.legend(title="numPMIDs", loc="upper left")
    plt.show()

def kmeans_full_analysis(df, features, k_list, use_3d):
    """
    df: pandas dataframe
    features: kÃ¼meleme yapÄ±lacak sayÄ±sal sÃ¼tunlar
    k_list: denenmek istenen k deÄŸerleri listesi
    use_3d: 3 deÄŸiÅŸken varsa ve 3D gÃ¶rselleÅŸtirme yapÄ±lmak isteniyorsa True yapÄ±labilir
    """

    # SayÄ±sal alanlarÄ± seÃ§ip standartlaÅŸtÄ±r
    X = df[features].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    for idx, k in enumerate(k_list):
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)
        df[f"cluster_{k}"] = clusters

        print(f"\n=== k = {k} iÃ§in kÃ¼me daÄŸÄ±lÄ±mÄ± ===")
        print(df[f"cluster_{k}"].value_counts())
        print("-" * 40)

        # 2D scatter (ilk 2 feature Ã¼zerinden)
        plt.figure(figsize=(7, 5))
        sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=clusters, palette="tab10", s=50)
        centers = kmeans.cluster_centers_
        plt.scatter(centers[:, 0], centers[:, 1], c="black", s=200, alpha=0.5, marker="X", label="Merkezler")
        plt.title(f"K-Means 2D KÃ¼meleme (k = {k})")
        plt.xlabel(f"{features[0]} (scaled)")
        plt.ylabel(f"{features[1]} (scaled)")
        plt.legend()
        plt.show()

        # 3D scatter (isteÄŸe baÄŸlÄ±)
        if use_3d and len(features) == 3:
            fig = plt.figure(figsize=(8, 6))
            ax = fig.add_subplot(111, projection="3d")
            scatter = ax.scatter(X_scaled[:, 0], X_scaled[:, 1], X_scaled[:, 2],
                                 c=clusters, cmap="tab10", s=50)
            ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2],
                       c="black", s=200, alpha=0.5, marker="X", label="Merkezler")
            ax.set_xlabel(f"{features[0]} (scaled)")
            ax.set_ylabel(f"{features[1]} (scaled)")
            ax.set_zlabel(f"{features[2]} (scaled)")
            ax.set_title(f"K-Means 3D KÃ¼meleme (k = {k})")
            plt.legend(*scatter.legend_elements(), title="KÃ¼me")
            plt.show()

        # Pairplot (eÄŸer 3 deÄŸiÅŸken varsa)
        if len(features) >= 2:
            prplt = sns.pairplot(df, vars=features, hue=f"cluster_{k}", palette="tab10")
            # BaÅŸlÄ±ÄŸÄ± biraz daha yukarÄ±ya al ve font ayarlarÄ±nÄ± yap
            prplt.fig.suptitle(f"K={k} iÃ§in Pairplot - SayÄ±sal DeÄŸiÅŸkenler ve KÃ¼me DaÄŸÄ±lÄ±mÄ±", y=1.03, fontsize=14, fontweight="bold")
            # plt.suptitle(f"K={k} iÃ§in Pairplot - SayÄ±sal DeÄŸiÅŸkenler ve KÃ¼me DaÄŸÄ±lÄ±mÄ±", y=1.02)
            # Alt grafikler ile baÅŸlÄ±k arasÄ±nda boÅŸluk bÄ±rak
            plt.subplots_adjust(top=0.92)
            plt.show()

        # KÃ¼me bazlÄ± Ã¶zet tablo
        print(f"\nKÃ¼me BazlÄ± Ortalama DeÄŸerler (k = {k}):\n")
        cluster_summary = df.groupby(f"cluster_{k}")[features].mean().round(3)
        print(cluster_summary)

    return df

def elbow(df):
    # X = df[["score", "ei", "numPMIDs"]].fillna(0)
    X = df[["score", "ei"]].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Ä°nertia deÄŸerlerini saklayacaÄŸÄ±mÄ±z liste
    inertia_list = []
    k_values = list(range(1, 11))  

    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X_scaled)
        inertia_list.append(kmeans.inertia_)
        #print(f"k: {k}",end=" ")
        #print(kmeans.inertia_)
    
    inertia_df = pd.DataFrame({"k": k_values, "inertia": inertia_list})
    print("\nInertia DeÄŸerleri Tablosu:\n")
    print(inertia_df)
    
    plt.figure(figsize=(8, 5))
    plt.plot(k_values, inertia_list, marker="o")
    plt.title("Elbow YÃ¶ntemi - Toplam Hata (Inertia) GrafiÄŸi")
    plt.xlabel("KÃ¼me SayÄ±sÄ± (k)")
    plt.ylabel("Toplam Hata (Inertia)")
    plt.xticks(k_values)
    plt.grid()
    plt.show()
    
    return inertia_df

def silhouette_analysis(df, k_list):
    """
    Silhouette skorlarÄ±nÄ± hesaplayÄ±p grafik ve tablo oluÅŸturur.

    Parametreler:
    - df: pandas dataframe
    - max_k: denenecek maksimum kÃ¼me sayÄ±sÄ± (minimum 2 olmalÄ±)
    """

    # X = df[["score", "ei", "numPMIDs"]].fillna(0)
    X = df[["score", "ei"]].fillna(0)
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    silhouette_results = []
 
    for k in k_list:
        print(f"\nk = {k} iÃ§in silhouette hesaplanÄ±yor...")
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X_scaled)
        score = silhouette_score(X_scaled, labels)
        print(f"Silhouette skoru: {score:.4f}")
        silhouette_results.append({"k": k, "silhouette_score": score})

    silhouette_df = pd.DataFrame(silhouette_results)
    print("\nSilhouette Skor Tablosu:\n")
    print(silhouette_df)

    plt.figure(figsize=(8, 5))
    plt.plot(silhouette_df["k"], silhouette_df["silhouette_score"], marker="o")
    plt.title("SeÃ§ilen k DeÄŸerleri iÃ§in Silhouette Skoru")
    plt.xlabel("KÃ¼me SayÄ±sÄ± (k)")
    plt.ylabel("Silhouette Skoru")
    plt.grid()
    plt.show()

    return silhouette_df

def hierarchical_clustering(df, n_clusters):

    X = df[["score", "ei"]].fillna(0)
    # X = df[["score", "ei", "numPMIDs"]].fillna(0)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # HiyerarÅŸik baÄŸlantÄ± matrisi (ward: varyans-minimizasyonu)
    Z = linkage(X_scaled, method='ward')

    # Dendrogram Ã§izimi
    plt.figure(figsize=(12, 6))
    dendrogram(Z, truncate_mode="level", p=10)
    plt.title("HiyerarÅŸik KÃ¼meleme DendrogramÄ±")
    plt.xlabel("Veri NoktasÄ±")
    plt.ylabel("UzaklÄ±k")
    plt.tight_layout()
    plt.show()

    df[f"cluster_hc_{n_clusters}"] = fcluster(Z, t=n_clusters, criterion='maxclust')

    print(f"{n_clusters} kÃ¼me iÃ§in daÄŸÄ±lÄ±m:")
    print(df[f"cluster_hc_{n_clusters}"].value_counts())

    return df

def dbscan_clustering_analysis(df, X, eps, min_samples):
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import DBSCAN
    import matplotlib.pyplot as plt
    import seaborn as sns

    print("[1/7] Veriler seciliyor ve standardize ediliyor...")
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    print("[2/7] DBSCAN uygulanÄ±yor...")
    db = DBSCAN(eps=eps, min_samples=min_samples)
    labels = db.fit_predict(X_scaled)

    df["dbscan_cluster"] = labels  # -1 olanlar "gÃ¼rÃ¼ltÃ¼"

    print("[3/7] DBSCAN KÃ¼me DaÄŸÄ±lÄ±mÄ±:")
    print(df["dbscan_cluster"].value_counts().sort_index())
    print("-" * 40)
    
    # ğŸ” GÃ¼rÃ¼ltÃ¼ Analizi
    noise = df[df["dbscan_cluster"] == -1]
    print(f"[3.1] GÃ¼rÃ¼ltÃ¼ (outlier) sayÄ±sÄ±: {len(noise)}")
    print(f"[3.2] GÃ¼rÃ¼ltÃ¼ oranÄ±: %{(len(noise) / len(df)) * 100:.4f}")
    if len(noise) > 0:
        print("[3.3] GÃ¼rÃ¼ltÃ¼ verileri (ilk 5 satÄ±r):")
        print(noise[["score", "ei"]].head())
        
        print("\n[3.4] GÃ¼rÃ¼ltÃ¼ verilerinin istatistiksel Ã¶zeti:")
        print(noise[["score", "ei"]].describe())

        noise.to_csv("noise_data.csv", index=False)
        print("\n[3.5] GÃ¼rÃ¼ltÃ¼ verileri 'noise_data.csv' olarak kaydedildi.")
    else:
        print("[3.3] GÃ¼rÃ¼ltÃ¼ verisi bulunamadÄ±.")
    print("-" * 40)
    
    print("[4/7] 2D Scatter Plot (score vs ei)...")
    plt.figure(figsize=(6, 6))
    sns.scatterplot(data=df, x="score", y="ei", hue="dbscan_cluster", palette="tab10", s=50)
    plt.title(f"DBSCAN Clustering (eps={eps}, min_samples={min_samples})")
    plt.xlabel("score")
    plt.ylabel("ei")
    plt.legend(title="KÃ¼me")
    plt.tight_layout()
    plt.show()

    print("[5/7] Boxplot'lar Ã§iziliyor...")
    for col in X.columns:
        plt.figure(figsize=(6, 4))
        sns.boxplot(data=df, x="dbscan_cluster", y=col, palette="Set3")
        plt.title(f"{col} DaÄŸÄ±lÄ±mÄ± (Boxplot)")
        plt.xlabel("KÃ¼me")
        plt.ylabel(col)
    plt.tight_layout()
    plt.show()

    print("[6/7] Histogramlar Ã§iziliyor...")
    for col in X.columns:
        plt.figure(figsize=(6, 4))
        sns.histplot(data=df, x=col, hue="dbscan_cluster", kde=True, palette="Set2", multiple="stack")
        plt.title(f"{col} HistogramÄ± (KÃ¼me bazlÄ±)")
        plt.xlabel(col)
    plt.tight_layout()
    plt.show()
    """
    print("[7/7] 3d scatter plot Ã§iziliyor...")
    fig = plt.figure(figsize=(8, 6))
    ax = fig.add_subplot(111, projection='3d')
    scatter = ax.scatter(df["score"], df["ei"], df["numPMIDs"],
                         c=df["dbscan_cluster"], cmap="tab10", s=20)
    ax.set_xlabel("score")
    ax.set_ylabel("ei")
    ax.set_zlabel("numPMIDs")
    ax.set_title(f"DBSCAN Clustering (3D) - eps={eps}, min_samples={min_samples}")
    legend = ax.legend(*scatter.legend_elements(), title="KÃ¼me")
    ax.add_artist(legend)
    plt.tight_layout()
    plt.show()
    """
    return df

def clean_text(text):
    if pd.isnull(text):
        return ""
    # KÃ¼Ã§Ã¼k harfe Ã§evir
    text = text.lower()
    # Parantez iÃ§ini temizle (Ã¶rneÄŸin "(C17)" gibi)
    text = re.sub(r"\([^\)]*\)", "", text)
    # Noktalama ve sayÄ± temizleme
    text = re.sub(r"[^a-z\s]", "", text)
    # Tokenize ve lemmatize
    words = text.split()
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))

    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]
    return " ".join(words)

def frequency_analysis(df):

    text_columns = [
        "diseaseName",
        "diseaseClasses_DO",
        "diseaseClasses_MSH",
        "diseaseClasses_UMLS_ST",
        "diseaseClasses_HPO"
    ]

    for col in text_columns:
        print(f"\n--- {col.upper()} sÃ¼tunu kelime frekansÄ± ---")

        all_text = " ".join(df[col].dropna())
        words = all_text.split()

        word_freq = Counter(words)
        most_common = word_freq.most_common(20)
        print(most_common)

        words, counts = zip(*most_common)
        plt.figure(figsize=(10, 5))
        plt.bar(words, counts)
        plt.title(f"{col} SÃ¼tununda En SÄ±k GeÃ§en 20 Kelime")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

        wordcloud = WordCloud(width=800, height=400, background_color="white").generate(all_text)
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        plt.title(f"{col} SÃ¼tunu - WordCloud")
        plt.show()

def tf_idf(df):
    
    # TF-IDF vektÃ¶rleÅŸtirici oluÅŸtur
    vectorizer = TfidfVectorizer(max_features=1000)  # En sÄ±k geÃ§en 1000 terim
    tfidf_matrix = vectorizer.fit_transform(df["diseaseName"])

    feature_names = vectorizer.get_feature_names_out()

    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
    tfidf_df.to_csv("filtered_tfidf_diseaseName.csv", index=False)

    mean_tfidf = tfidf_df.mean().sort_values(ascending=False)
    top_n = 20
    top_words = mean_tfidf[:top_n]

    plt.figure(figsize=(10, 5))
    top_words.plot(kind="bar", color="skyblue")
    plt.title("En AnlamlÄ± 20 Terim (TF-IDF)")
    plt.ylabel("Ortalama TF-IDF Skoru")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig("tfidf_top20_terms.png")
    plt.show()
    
def gene_disease(df):
    """co-occurance"""
    # 1. En sÄ±k birlikte gÃ¶rÃ¼len genâ€“hastalÄ±k Ã§iftleri
    pair_freq = df.groupby(["symbolOfGene", "diseaseName"]).size().reset_index(name="count")
    pair_freq_sorted = pair_freq.sort_values(by="count", ascending=False)
    print(pair_freq_sorted.head(10))

    # 2. Gen baÅŸÄ±na hastalÄ±k sayÄ±sÄ±
    gene_disease_count = df.groupby("symbolOfGene")["diseaseName"].nunique().sort_values(ascending=False)
    print(gene_disease_count.head(10))

    # 3. HastalÄ±k baÅŸÄ±na gen sayÄ±sÄ±
    disease_gene_count = df.groupby("diseaseName")["symbolOfGene"].nunique().sort_values(ascending=False)
    print(disease_gene_count.head(10))
    
    top_pairs = pair_freq_sorted.head(20)

    plt.figure(figsize=(10,6))
    sns.barplot(data=top_pairs, x="count", y=top_pairs["symbolOfGene"] + " â†” " + top_pairs["diseaseName"])
    plt.title("En SÄ±k GÃ¶rÃ¼len Gen-HastalÄ±k EÅŸleÅŸmeleri")
    plt.xlabel("Frekans")
    plt.ylabel("Gen â†” HastalÄ±k")
    plt.tight_layout()
    plt.show()

def network_analysis(df):
    edges = df[["symbolOfGene", "diseaseName"]].dropna()
    edges = edges.drop_duplicates()

    # Bipartite graph 
    B = nx.Graph()
    B.add_nodes_from(edges["symbolOfGene"], bipartite="genes")
    B.add_nodes_from(edges["diseaseName"], bipartite="diseases")
    B.add_edges_from(list(edges.itertuples(index=False, name=None)))

    print(f"Toplam dÃ¼ÄŸÃ¼m sayÄ±sÄ±: {B.number_of_nodes()}")
    print(f"Toplam baÄŸlantÄ± sayÄ±sÄ±: {B.number_of_edges()}") 
    
    gene_nodes = {n for n, d in B.nodes(data=True) if d["bipartite"] == "genes"}
    disease_nodes = set(B) - gene_nodes

    # genler arasÄ± baÄŸlantÄ±lar
    G_genes = bipartite.projected_graph(B, gene_nodes)
    print(f"Gen projeksiyonundaki dÃ¼ÄŸÃ¼m sayÄ±sÄ±: {G_genes.number_of_nodes()}")
    print(f"Gen projeksiyonundaki baÄŸlantÄ± sayÄ±sÄ±: {G_genes.number_of_edges()}") 
    """
    # --- 1. Etiketsiz tÃ¼m aÄŸ yapÄ±sÄ± (karmaÅŸÄ±k olsa da genel gÃ¶rÃ¼nÃ¼m)
    plt.figure(figsize=(10, 8))
    pos = nx.spring_layout(B, k=0.1, iterations=10)
    nx.draw(B, pos, node_size=5, with_labels=False, edge_color="gray", alpha=0.4)
    plt.title("TÃ¼m Gen-HastalÄ±k AÄŸÄ± (Etiketsiz YapÄ±)")
    plt.show()

    # --- 2. En Ã§ok geÃ§en 50 genin oluÅŸturduÄŸu alt aÄŸÄ± Ã§iz
    top_genes = df["symbolOfGene"].value_counts().head(50).index.tolist()
    sub_edges = [(g, d) for g, d in B.edges() if g in top_genes]
    subgraph = nx.Graph()
    subgraph.add_edges_from(sub_edges)
    
    plt.figure(figsize=(14, 10))
    pos = nx.spring_layout(subgraph, seed=42)
    nx.draw(subgraph, pos, node_color="skyblue", node_size=80, with_labels=False, font_size=7)
    plt.title("En SÄ±k GeÃ§en 50 Genin HastalÄ±klarla Alt AÄŸÄ±")
    plt.show()
    """
    # Sadece ilk 100 kenarla 
    sub_nodes = list(edges.head(100)["symbolOfGene"]) + list(edges.head(100)["diseaseName"])
    subgraph = B.subgraph(sub_nodes)

    plt.figure(figsize=(12, 8))
    nx.draw(subgraph, with_labels=True, node_color="skyblue", node_size=300, font_size=8)
    plt.title("Gen-HastalÄ±k Bipartite Network (Ä°lk 100 Kenar)")
    plt.show()
